{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8: Backtesting\n",
    "\n",
    "In this project, you will build a fairly realistic backtester that uses the Barra data. The backtester will perform portfolio optimization that includes transaction costs, and you'll implement it with computational efficiency in mind, to allow for a reasonably fast backtest. You'll also use performance attribution to identify the major drivers of your portfolio's profit-and-loss (PnL). You will have the option to modify and customize the backtest as well.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "\n",
    "## Packages\n",
    "\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==2.1.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Collecting numpy==1.16.1 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 2.3MB/s eta 0:00:01   13% |████▍                           | 2.4MB 23.3MB/s eta 0:00:01    26% |████████▌                       | 4.6MB 22.8MB/s eta 0:00:01    33% |██████████▋                     | 5.7MB 22.8MB/s eta 0:00:01    80% |█████████████████████████▊      | 13.9MB 26.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.24.1 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/de/a0d3defd8f338eaf53ef716e40ef6d6c277c35d50e09b586e170169cdf0d/pandas-0.24.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 5.1MB/s eta 0:00:01   10% |███▍                            | 1.1MB 23.1MB/s eta 0:00:01    23% |███████▌                        | 2.4MB 25.8MB/s eta 0:00:01    60% |███████████████████▍            | 6.1MB 23.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting patsy==0.5.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/0c/5f61f1a3d4385d6bf83b83ea495068857ff8dfb89e74824c6e9eb63286d8/patsy-0.5.1-py2.py3-none-any.whl (231kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 19.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scipy==0.19.1 (from -r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/46/da8d7166102d29695330f7c0b912955498542988542c0d2ae3ea0389c68d/scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 48.2MB 719kB/s eta 0:00:01  2% |▊                               | 1.1MB 23.0MB/s eta 0:00:03    11% |███▉                            | 5.8MB 24.9MB/s eta 0:00:02    19% |██████▏                         | 9.3MB 25.2MB/s eta 0:00:02    26% |████████▌                       | 12.8MB 25.4MB/s eta 0:00:02    28% |█████████▎                      | 13.9MB 19.6MB/s eta 0:00:02    33% |██████████▊                     | 16.2MB 24.1MB/s eta 0:00:02    42% |█████████████▊                  | 20.7MB 18.1MB/s eta 0:00:02    44% |██████████████▍                 | 21.6MB 18.5MB/s eta 0:00:02    46% |███████████████                 | 22.6MB 20.5MB/s eta 0:00:02    48% |███████████████▌                | 23.4MB 15.7MB/s eta 0:00:02    51% |████████████████▌               | 24.9MB 16.7MB/s eta 0:00:02    54% |█████████████████▋              | 26.5MB 15.7MB/s eta 0:00:02    56% |██████████████████              | 27.2MB 16.3MB/s eta 0:00:02    57% |██████████████████▋             | 28.0MB 16.2MB/s eta 0:00:02    61% |███████████████████▋            | 29.5MB 14.8MB/s eta 0:00:02    62% |████████████████████            | 30.2MB 15.5MB/s eta 0:00:02    64% |████████████████████▊           | 31.2MB 16.0MB/s eta 0:00:02    66% |█████████████████████▎          | 32.0MB 16.3MB/s eta 0:00:01    68% |█████████████████████▉          | 32.9MB 16.0MB/s eta 0:00:01    71% |███████████████████████         | 34.5MB 20.3MB/s eta 0:00:01    77% |████████████████████████▉       | 37.4MB 19.0MB/s eta 0:00:01    79% |█████████████████████████▋      | 38.5MB 22.4MB/s eta 0:00:01    84% |███████████████████████████     | 40.6MB 16.1MB/s eta 0:00:01    86% |███████████████████████████▊    | 41.8MB 23.6MB/s eta 0:00:01    90% |█████████████████████████████▏  | 43.9MB 21.1MB/s eta 0:00:01    93% |█████████████████████████████▉  | 44.9MB 18.1MB/s eta 0:00:01    96% |███████████████████████████████ | 46.7MB 15.7MB/s eta 0:00:01    98% |███████████████████████████████▊| 47.7MB 22.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting statsmodels==0.9.0 (from -r requirements.txt (line 6))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d1/69ee7e757f657e7f527cbf500ec2d295396e5bcec873cf4eb68962c41024/statsmodels-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.4MB 7.2MB/s eta 0:00:01   6% |██                              | 450kB 22.8MB/s eta 0:00:01    34% |███████████                     | 2.6MB 22.4MB/s eta 0:00:01    61% |███████████████████▋            | 4.5MB 20.3MB/s eta 0:00:01    75% |████████████████████████▎       | 5.6MB 27.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.19.5 (from -r requirements.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 6.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.1.0->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.1.0->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.1.0->-r requirements.txt (line 1)) (2017.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib==2.1.0->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.1.0->-r requirements.txt (line 1)) (2.2.0)\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "\u001b[31mmoviepy 0.2.3.2 has requirement tqdm==4.11.2, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, pandas, patsy, scipy, statsmodels, tqdm\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "  Found existing installation: pandas 0.23.3\n",
      "    Uninstalling pandas-0.23.3:\n",
      "      Successfully uninstalled pandas-0.23.3\n",
      "  Found existing installation: patsy 0.4.1\n",
      "    Uninstalling patsy-0.4.1:\n",
      "      Successfully uninstalled patsy-0.4.1\n",
      "  Found existing installation: scipy 1.2.1\n",
      "    Uninstalling scipy-1.2.1:\n",
      "      Successfully uninstalled scipy-1.2.1\n",
      "  Found existing installation: statsmodels 0.8.0\n",
      "    Uninstalling statsmodels-0.8.0:\n",
      "      Successfully uninstalled statsmodels-0.8.0\n",
      "  Found existing installation: tqdm 4.11.2\n",
      "    Uninstalling tqdm-4.11.2:\n",
      "      Successfully uninstalled tqdm-4.11.2\n",
      "Successfully installed numpy-1.16.1 pandas-0.24.1 patsy-0.5.1 scipy-0.19.1 statsmodels-0.9.0 tqdm-4.19.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import patsy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import median\n",
    "from scipy.stats import gaussian_kde\n",
    "from statsmodels.formula.api import ols\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We’ll be using the Barra dataset to get factors that can be used to predict risk. Loading and parsing the raw Barra data can be a very slow process that can significantly slow down your backtesting. For this reason, it's important to pre-process the data beforehand. For your convenience, the Barra data has already been pre-processed for you and saved into pickle files. You will load the Barra data from these pickle files.\n",
    "\n",
    "In the code below, we start by loading `2004` factor data from the `pandas-frames.2004.pickle` file. We also load the `2003` and `2004` covariance data from the `covaraince.2003.pickle`  and `covaraince.2004.pickle` files. You are encouraged  to customize the data range for your backtest. For example, we recommend starting with two or three years of factor data. Remember that the covariance data should include all the years that you choose for the factor data,   and also one year earlier. For example, in the code below we are using  `2004` factor data, therefore, we must include `2004` in our covariance data, but also the previous year, `2003`. If you don't remember why must include this previous year, feel free to review the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "barra_dir = '../../data/project_8_barra/'\n",
    "\n",
    "data = {}\n",
    "for year in [2004]:\n",
    "    fil = barra_dir + \"pandas-frames.\" + str(year) + \".pickle\"\n",
    "    data.update(pickle.load( open( fil, \"rb\" ) ))\n",
    "    \n",
    "covariance = {}\n",
    "for year in [2003, 2004]:\n",
    "    fil = barra_dir + \"covariance.\" + str(year) + \".pickle\"\n",
    "    covariance.update(pickle.load( open(fil, \"rb\" ) ))\n",
    "    \n",
    "daily_return = {}\n",
    "for year in [2004, 2005]:\n",
    "    fil = barra_dir + \"price.\" + str(year) + \".pickle\"\n",
    "    daily_return.update(pickle.load( open(fil, \"rb\" ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Daily Returns Data (TODO)\n",
    "\n",
    "In the cell below, we want to incorporate a realistic time delay that exists in live trading, we’ll use a two day delay for the `daily_return` data. That means the `daily_return` should be two days after the data in `data` and `cov_data`. Combine `daily_return` and `data` together in a dict called `frames`.\n",
    "\n",
    "Since reporting of PnL is usually for the date of the returns, make sure to use the two day delay dates (dates that match the `daily_return`) when building `frames`. This means calling `frames['20040108']` will get you the prices from \"20040108\" and the data from `data` at \"20040106\".\n",
    "\n",
    "Note: We're not shifting `covariance`, since we'll use the \"DataDate\" field in `frames` to lookup the covariance data. The \"DataDate\" field contains the date when the `data` in `frames` was recorded. For example, `frames['20040108']` will give you a value of \"20040106\" for the field \"DataDate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barra_dir = '../../data/project_8_barra/'\n",
    "\n",
    "data = {}\n",
    "for year in [2006]:\n",
    "    fil = barra_dir + \"pandas-frames.\" + str(year) + \".pickle\"\n",
    "    data.update(pickle.load( open( fil, \"rb\" ) ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data\n",
    "dr=daily_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.update(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames ={}\n",
    "dlyreturn_n_days_delay = 2\n",
    "\n",
    "# TODO: Implement\n",
    "frames=data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Daily Returns date column (Optional)\n",
    "Name the column `DlyReturnDate`.\n",
    "**Hint**: create a list containing copies of the date, then create a pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorize\n",
    "\n",
    "As we have done in other projects, we'll want to avoid extremely positive or negative values in our data. Will therefore create a function, `wins`, that will clip our values to a minimum and maximum range. This process is called **Winsorizing**. Remember that this helps us handle noise, which may otherwise cause unusually large positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wins(x,a,b):\n",
    "    return np.where(x <= a,a, np.where(x >= b, b, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot\n",
    "\n",
    "Let's check our `wins` function by taking a look at the distribution of returns for a single day `20040102`. We will clip our data from `-0.1` to `0.1` and plot it using our `density_plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8XGd97/HPT/suWZslb5J3x85iJ46zkcQJJIEECpT2kgBNWEoul3LL9rq9KS23y6W99PYSCoVCkzSF0EKhAUJoAklIs0FW27FjO14ly6us1dp3zXP/OGecQUjWSJqZM6P5vl8vvTRz5pw5Px95zm+e5zzn95hzDhERSV8ZQQcgIiLBUiIQEUlzSgQiImlOiUBEJM0pEYiIpDklAhGRNKdEICKS5pQIRETSnBKBiEiaywo6gGhUVla6+vr6oMMQEUkp27dvb3fOVU23Xkokgvr6erZt2xZ0GCIiKcXMjkaznrqGRETSnBKBiEiaUyIQEUlzSgQiImlOiUBEJM0pEYiIpDklAhGRNKdEIDJLzjmePdjGo7ubgw5FZE5S4oYykWTTOzTKnQ9s54XGDgA+dFU9f3rLejIzLODIRGZOiUBkFu59tpEXGjv4s3es53jnIPf/6ghLFhTwkTctDzo0kRlTIhCZofa+Ye775RFuuaCWD13lnfj3nurmvucauf2KOrIz1eMqqUX/Y0Vm6B+faWB4LMRnblxzdtnHrl1Jc/cQP911KsDIRGZHiUBkBkIhx0M7T3Hj+oWsrCo6u3zr2irWLCzi3ueOBBidyOwoEYjMwM4TXbT1DnPThppfW25m3HrpMvY193C0oz+g6ERmR4lAZAaeeL2FrAzjurXVv/Ha9eu8ZU/tb010WCJzokQgMgOP7z3N5SsqKC3I/o3X6isLWVFZyH8eaAsgMpHZUyIQiVJjWx8Nbf3cuGHhlOtct66aFxs7GBgZS2BkInOjRCASpZePdALwplWVU65z/bpqRsZC/OpwR6LCEpkzJQKRKG07eobywhyWVxZOuc6l9eXkZmXwYqMSgaQOJQKRKO04eoaLly3AbOoyEjlZGVy4pJQdx84kMDKRuVEiEIlCZ/8Ije39XFK3YNp1L162gL0nexgeG09AZCJzp0QgEoUdR71v+NEkgk3LyhgZD7HnZE+8wxKJCSUCkShsP3aGrAzjwiWl06578TIvWbyq7iFJEUoEIlF49dgZNiwqIS87c9p1q0vyWFyWz6vHuhIQmcjcKRGITMM5x+unetiwePrWQNimZWW6YCwpI26JwMyWmtlTZrbPzPaa2Sf95X9uZifNbKf/c3O8YhCJhVPdQ/QMjXFebUnU22xcWkZz9xDtfcNxjEwkNuI5H8EY8Fnn3A4zKwa2m9kT/mtfds79vzjuWyRm9p3yLvqury2Oeptw0tjX3MPVq6viEpdIrMStReCca3bO7fAf9wL7gMXx2p9IvOxr9hLB2proWwSRiUAk2SXkGoGZ1QObgJf8RZ8ws9fM7H4zm348nkiA9p3uoa6igKLc6BvQ5YU5LCzJZX9zbxwjE4mNuCcCMysCfgh8yjnXA3wDWAlsBJqBL02x3Z1mts3MtrW1qZqjBGdfcy/raqLvFgo7r7aE19UikBQQ10RgZtl4SeBfnXM/AnDOtTjnxp1zIeBeYMtk2zrn7nHObXbOba6qUh+rBGNgZIymjv4ZXSgOO6+2hIa2PkbGQnGITCR24jlqyIB/AvY55+6OWF4bsdq7gT3xikFkrg6c7sU5Zp0IRscdh1v74hCZSOzEc9TQVcDvAbvNbKe/7HPAbWa2EXBAE/Bf4xiDyJwcavFO4msXzrxrKDzKaF9zD+sXzTyRiCRK3BKBc+6XwGRlGh+N1z5FYq2hrY+czAyWlhfMeNvllUXkZmWw/7SuE0hy053FIudwuLWP5ZWFZGZMXXp6KpkZxoqqInUNSdJTIhA5h4a2PlZVF816+9XVRRxSIpAkp0QgMoXhsXGOdQ6wsmrqGcmms6q6iJNdg5rDWJKaEoHIFJraBwg5WDmHFsGq6iKcg8a2/hhGJhJbSgQiU2ho87p0VlbNrWsI0HUCSWpKBCJTCJ+8V8yha6iuwrvQrEQgyUyJQGQKDW19LC7LpyBn9qOsc7IyqK8o4FCrag5J8lIiEJlCQ1vfnFoDYauqNYRUkpsSgcgknHMc7RhgeeXcE8Hq6mKaOgZUc0iSlhKByCS6B0fpHRpj2SzuKJ5oRVUh4yHH8TMDMYhMJPaUCEQmcazTO2nPprTERPV+q6KpXUNIJTkpEYhMIpwIYtIi8BPBESUCSVJKBCKTiGUiKCvIoawgW4lAkpYSgcgkjncOUFmUQ+EMpqc8l/qKQiUCSVpKBCKTONoxEJPrA2ErKgt1jUCSlhKByCSOdQ7EpFsorL6ykFPdQwyNjsfsPUViRYlAZILR8RCnugZjnggAmjrUKpDko0QgMsGprkFCLjZDR8NWaAipJDElApEJwiOG6uLQImhUIpAkpEQgMsHZoaMVsUsERblZVBXnqkUgSUmJQGSCY50D5GRmsLA4L6bvu7yikKZ2lZmQ5KNEIDLBsY4BlpTnkzGLCevPpb6yQF1DkpSUCEQmiPXQ0bDllUW09w3TOzQa8/cWmQslApEIzjmOdcQrEXjvqe4hSTZKBCIRugdH6R2OTfnpicIjh47oXgJJMkoEIhFiWWxuovoK3UsgyUmJQCRCPIaOhuVlZ7KoNE/F5yTpKBGIRDja4U9IsyD2iQC87iElAkk2cUsEZrbUzJ4ys31mttfMPukvLzezJ8zskP97QbxiEJmpWJefnmh5ZaHqDUnSiWeLYAz4rHPuPOBy4A/MbD1wF/Ckc2418KT/XCQpHOuMbfnpiZZXFtI1MMqZ/pG47UNkpuKWCJxzzc65Hf7jXmAfsBh4J/Btf7VvA++KVwwiMxWvewjCzl4wVqtAkkhCrhGYWT2wCXgJWOicawYvWQDVU2xzp5ltM7NtbW1tiQhT0ly4/HQsi81NVB++l0CJQJJI3BOBmRUBPwQ+5ZzriXY759w9zrnNzrnNVVVV8QtQxBeP8tMTLS0vwEw3lUlyiWsiMLNsvCTwr865H/mLW8ys1n+9FmiNZwwi0QqPGIpn11BuViaLSvPVIpCkEs9RQwb8E7DPOXd3xEsPA3f4j+8AfhKvGERmIp73EETyRg6pRSDJI54tgquA3wOuN7Od/s/NwBeBG8zsEHCD/1wkcMfjVH56orqKAo6qRSBJJD6DpQHn3C+Bqer4vjle+xWZrWOd8Sk/PVF4CGnXwAhlBTlx3ZdINHRnsYgv3kNHw+rODiFV95AkByUCEd4oPx3PoaNh9RXhctTqHpLkoEQgAnQNeOWn4zl0NOzsEFJdJ5AkoUQgQnzLT0/kVSHNV4tAkoYSgQiJGzoaVl9ZoGsEkjSUCER4IxHEq/z0RHUVhRpCKklDiUCE+Jefnmh5RSFnBkbpHtBE9hI8JQIR4l9+eqK6ChWfk+ShRCCCV2coEUNHw8IT2SsRSDJQIpC0NzIWorl7MCEjhsKWqQqpJBElAkl7iSg/PVFedia1JXlqEUhSUCKQtJfIewgi1Wv+YkkSSgSS9hJ9D0GYN4RUXUMSPCUCSXvHOwfIyYp/+emJ6isK6OwfoXtQQ0glWEoEkvaOdQ6wdEH8y09PFB45pBvLJGhKBJL2jnYkpvz0RPV+OeojqjkkAVMikLTmnON4guYhmCh8U5muE0jQlAgkrSWy/PREedmZ1JbmqQqpBE6JQNJaUENHw+orNIRUghdVIjCzH5rZLWamxCHzSlBDR8PqKwvUNSSBi/bE/g3gfcAhM/uima2LY0wiCZPo8tMT1VUU0tE/Qs+QhpBKcKJKBM65Xzjn3g9cDDQBT5jZ82b2ITPLjmeAIvF0rGOAyqLchJWfnig8cuioag5JgKLu6jGzCuCDwO8DrwJfwUsMT8QlMpEEONY5wLLy/MD2X1/ptUSO6DqBBCiqr0Fm9iNgHfAd4B3OuWb/pe+b2bZ4BScSb8c6B7i0fkFg+68rD7cIlAgkONG2h+9zzj0aucDMcp1zw865zXGISyTu3ig/vTiwGPJzMqkpyVOLQAIVbdfQFyZZ9kIsAxFJtCDKT09meWUhjW1KBBKcc7YIzKwGWAzkm9kmIFyMpQQI9tMjMkdB30MQtqq6iId2nsQ5h1li6x2JwPRdQzfhXSBeAtwdsbwX+Ny5NjSz+4G3A63OufP9ZX8OfBRo81f73MQuJ5FEORrwPQRhK6sK6R0ao613mOqSxFZAFYFpEoFz7tvAt83sPc65H87wvb8FfA14YMLyLzvn/t8M30sk5o519JMbQPnpiVZVFwNwuK1PiUACMV3X0Aecc/8C1JvZZya+7py7e5LNwq89a2b1c45QJE6aOgaoqyhIePnpiVZWeyOHGlr7uHJlZaCxSHqa7mJxof+7CCie5Gc2PmFmr5nZ/WY25bg9M7vTzLaZ2ba2trapVhOZtaMd/dRVFE6/YpzVlORRmJNJgy4YS0Cm6xr6R//3X8Rof98A/jfg/N9fAj48xb7vAe4B2Lx5s4vR/kUACIUcRzsGuHZNVdChYGasrC7icGtf0KFImoq26Nz/NbMSM8s2syfNrN3MPjDTnTnnWpxz4865EHAvsGWm7yESCy29QwyPhZKiRQCwqqqIhjYlAglGtPcR3Oic68EbBXQCWAP8j5nuzMxqI56+G9gz0/cQiYXwrGD1SZIIVlYX0dw9RN/wWNChSBqK9s7icGG5m4HvOec6pxvvbGbfA7YClWZ2AvgzYKuZbcTrGmoC/ussYhaZs3Dp57qAh46GrawqAqCxrY8Ll5QFHI2km2gTwU/NbD8wCHzczKqAoXNt4Jy7bZLF/zTD+ETioqmjn5zMDBaVBVdwLtIqf+TQ4VYlAkm8aMtQ3wVcAWx2zo0C/cA74xmYSDwdbR9gaXk+mQEPHQ1bVl5IZobpOoEEYiZF2M/Du58gcpuJN4uJpISmjv6kuT4AkJOVQV1FgUYOSSCiLUP9HWAlsBMY9xc7lAgkBTnnDR1Ntpu3VlYV6V4CCUS0LYLNwHrnnMbzS8pr6x1mcHT87KQwyWJVdRFPH2hldDxEdqamB5fEifZ/2x6gJp6BiCRK09kRQ8nTNQRei2B03J2tiiqSKNG2CCqB183sZWA4vNA591txiUokjpo6wvcQJF+LALyaQ+HhpCKJEG0i+PN4BiGSSE3t/WRlGIuTZOho2IoqfwhpWx83BhyLpJeoEoFz7hkzqwNWO+d+YWYFQGZ8QxOJj6MdAywtLyAryfrhS/KyWViSS0OrLhhLYkVba+ijwIPAP/qLFgMPxSsokXhq6uhPmjuKJ1pVXcSh1t6gw5A0E+1Xoj8ArgJ6AJxzh4DqeAUlEi/hoaPJdA9BpLULSzjY0st4SAP0JHGiTQTDzrmR8BP/pjL9T5WU09E/Qt/wWNK2CNbVFDM0GtLIIUmoaBPBM2b2ObxJ7G8A/h34afzCEomPox3JVXV0onW13nxP+5t7Ao5E0km0ieAuvAnnd+NVDH0U+NN4BSUSL03tyVV1dKLV1cWYwf7Tuk4giRPtqKGQmT0EPOSc07yRkrKaOvrJMFiyIDkTQX5OJssrCtl/Wi0CSZxztgjM8+dm1g7sBw6YWZuZ/a/EhCcSW41t3jzFOVnJNXQ00tqaYg6oRSAJNN2n4VN4o4Uudc5VOOfKgcuAq8zs03GPTiTGDrf2sbIqOa8PhK2rKeFo5wADI5qtTBJjukRwO3Cbc+5IeIFzrhH4gP+aSMoYDzmOtPcnffmGtTXFOAcHW1SSWhJjukSQ7Zxrn7jQv06QPcn6IknrxJkBRsZDSZ8INiwqAeD1U7pOIIkxXSIYmeVrIkknPPvXyurk7hpasiCfkrws9pzqDjoUSRPTjRq6yMwm+1piQF4c4hGJm3ANnxWVyd0iMDPOX1zK3pNKBJIY52wROOcynXMlk/wUO+fUNSQppaGtj4rCHBYU5gQdyrTOX1zKvtO9jI6Hgg5F0kDyjqETibGGtj5WVid3ayBsw6ISRsZCmsNYEkKJQNLG4RSa8GXDolIA9qh7SBJAiUDSQkffMGcGRpP+HoKw5ZWFFORkslcjhyQBlAgkLYTH5K+tKQ44kuhkZhjra0t47URX0KFIGlAikLRwwK/ds3ZhaiQCgIuWlrH3VI8uGEvcKRFIWjjQ0kdZQTZVxblBhxK1jUvLGB4Lsb9ZdYckvuKWCMzsfjNrNbM9EcvKzewJMzvk/14Qr/2LRDrY0suahcWYWdChRG3TsjIAXj1+JuBIZL6LZ4vgW8BbJyy7C3jSObcaeNJ/LhJXzjkOnu5NqW4hgMVl+VQV57LzmK4TSHzFLRE4554FOicsfifwbf/xt4F3xWv/ImHN3UP0Do+xJkUuFIeZGZuWlvHqcSUCia9EXyNY6JxrBvB/V0+1opndaWbbzGxbW5vmwpHZO9Di9bGnWosAYOOyMo6093OmX6W9JH6S9mKxc+4e59xm59zmqqqqoMORFHbQn+RlzcLUuJks0qal3mW0nWoVSBwlOhG0mFktgP+7NcH7lzS0/3QvC0tyKStI/hpDE120tJSsDOPlpom9rCKxk+hE8DBwh//4DuAnCd6/pKG9p7rPlmxINQU5WVy4pJSXjygRSPzEc/jo94AXgLVmdsLMPgJ8EbjBzA4BN/jPReJmaHSchrb+s5O9pKItyyt47UQXgyPjQYci89R08xHMmnPutileenO89iky0YHTvYyHHOtrUzcRXLainG8+08Crx85w5arKoMOReShpLxaLxEK4aFuqdg0BbK5bQIbBi+oekjhRIpB5be+pborzslhanh90KLNWnJfNhkWlvNTYEXQoMk8pEci8tvdUD+trS1KqtMRkrlhZwavHuhgYGQs6FJmHlAhk3hoPOfaf7knpbqGwa1ZXMTIe4kW1CiQOlAhk3jrc2sfQaIjzF6fuheKwzfULyMvO4NmD7UGHIvOQEoHMWzv9qp0bl5YFHMnc5WVncvmKCp49qHIrEntKBDJvvXqsi9L8bJZXpsb0lNO5ZnUVje39HO8cCDoUmWeUCGTe2nm8i4uWlqX8heKwa9Z4NbeeVqtAYkyJQOal/uExDrb0smkedAuFrawqZHllIY/vPR10KDLPKBHIvPTaiW5CzivjPF+YGTduWMgLDR10D44GHY7MI0oEMi+Fp3fcuGT+JAKAmzbUMBZyPLVfhXsldpQIZF7acbSL5ZWFLChMvdLT57JxSRnVxbk8pu4hiSElApl3QiHHK02dbKkvDzqUmMvIMG7aUMNTB1rpG9ZdxhIbSgQy7+w/3Uv34CiXrZh/iQDgnRsXMTQa4rE9ahVIbCgRyLzz0hGvDMNlKyoCjiQ+LqlbwNLyfB7aeTLoUGSeUCKQeeelxk6WLMhncVnqVhw9FzPj3RsX86vD7bT0DAUdjswDSgQyrzjneLmpk8uWz8/WQNi7Ni0m5OBHO9QqkLlTIpB55UBLL539I/P2+kDYiqoitiwv57svH2U85IIOR1KcEoHMK88c8MovXL16/k/pePsVdRzvHFQhOpkzJQKZV5452Ma6mmJqS+fn9YFIN66voao4lwdeaAo6FElxSgQyb/QNj/FKUyfX+sXZ5rucrAxu27KMpw60cailN+hwJIUpEci88fzhdkbHHdeuTY9EAPDBK+vJz87kG880BB2KpDAlApk3nj7YRkFOJpvr5veF4kjlhTnctmUZP9l5SvMUyKwpEci8EAo5nni9hWtWV5GTlV7/rT96zXIyDL7+1OGgQ5EUlV6fGJm3th09Q1vvMDdfWBt0KAlXW5rPBy6v4wfbjutagcyKEoHMC4/ubiYnK4Pr11UHHUog/vv1qynMyeKLP9sfdCiSgpQIJOWFQo6f7znN1jVVFOVmBR1OIMoLc/j4dat4cn8rTx3QXAUyM4EkAjNrMrPdZrbTzLYFEYPMH9uPneF0zxA3X5B+3UKRPvymelZWFfL5h/YwODIedDiSQoJsEVznnNvonNscYAwyD/zgleMU5mRyw/qFQYcSqNysTP7q3Rdw4swgdz9xIOhwJIWoa0hSWt/wGI/sbuYdFy2iME27hSJdvqKC9122jPt+eYTnG9qDDkdSRFCJwAGPm9l2M7tzshXM7E4z22Zm29raVEtFJvfIa6cYGBnnv1y6NOhQksaf3nIeyysK+ewPdtHZPxJ0OJICgkoEVznnLgbeBvyBmV0zcQXn3D3Ouc3Ouc1VVelzp6hEzznHd186xqrqIjYtnV+T1M9FQU4WX7l1Ex39I/z37+1gbDwUdEiS5AJJBM65U/7vVuDHwJYg4pDU9krTGXad6OaDV9ZjZkGHk1QuWFLKX73rfH51uIO/flRDSuXcEp4IzKzQzIrDj4EbgT2JjkNS3z3PNlBemMN7Ll4SdChJ6Xc3L+XDVy3n/l8d4b7nGoMOR5JYEFfXFgI/9r/BZQHfdc79PIA4JIUdbu3lF/ta+eSbV5Ofkxl0OEnrT245j+buQb7wyD7yczJ5/2V1QYckSSjhicA51whclOj9yvxy9xMHKczJ5PYrdGI7l8wM48vv3cjw2A7+5Md7GBoN8ZE3LQ86LEkyGj4qKWfX8S4e3X2a3796BRVFuUGHk/TysjP55gcu4W3n1/C//+N1FaeT36BEICnFOcf/+dk+ygtz+Og1K4IOJ2XkZGXw97dt4p0bF/G3jx3gcz/ezciYRhOJR3fgSEr50Y6TvNjYyRfedX7a1hWarazMDO7+LxtZVJbPN55u4ODpXv7hAxdTXZwXdGgSMLUIJGV09o/whUde5+JlZbxvy7Kgw0lJmRnG/3zrOr72vk3sPdXDO/7+l7zQ0BF0WBIwJQJJCc45/ujB1+gbHuOvf/sCMjJ038BcvP3CRfzo41dSkJPFbfe+yF/+9HWGRlWoLl0pEUhK+NbzTfxiXwt3ve081tWUBB3OvHBebQmP/OGbuP2KOu7/1RFu+epzvNLUGXRYEgAlAkl6vzrczl89so+3nFfNh6+qDzqceaUgJ4u/fOf5fOcjWxgcGed3v/kCn/3BLtp6h4MOTRJIiUCS2oHTvXzsX7azoqqQu9+7UaUk4uTq1VX84rPX8vGtK3l410mu/9LTfPOZBs1rkCaUCCRpHWrp5f33vUh+dib3f/BSSvKygw5pXivIyeKP3rqOn3/qGi6pW8AXf7afa/72Kb7zQpOGms5zSgSSlHYd7+LWe17EzPjenZezZEFB0CGljZVVRXzrQ1v4949dwfKKQj7/k71c/6Wn+cG240oI85Q554KOYVqbN29227ZpRst08fM9p/n093dSUZTDAx/ewoqqoqBDSlvOOZ491M7fPrafPSd7qCnJ40NX1XPbZcvUQksBZrY9mlkglQgkaQyPjXP34wf5x2cbuWhpGffefoludkoSzjmePtjGvc828nxDB0W5WbzvsmXcfkWdWmtJTIlAUsrO4138j3/fxaHWPj5w+TI+//b15Gapqmgy2n2im3ufa+SR3c2EnGPrmired1kd162tIitTvc3JRIlAUkLv0Ch//5+Hue+5RhaW5PHXv30B162tDjosicLJrkG+//Ix/u2V47T2DlNTksd7L13Kb1+8mLqKwqDDSzmtvUPsOt7NodZeugZGcc5RnJfNb120iPrK2R1PJQJJaiNjIb770lG++p+H6ewf4bYtS/njm89Tv3MKGhsP8eT+Vv71pWM8d6gN52DTsjLetXExb7+wVhViz6FveIwf7TjBg9tP8NqJ7rPL87IzMIzB0XG+85EtXL16dtP1KhFIUhoeG+dHO07yjacbONY5wBUrKvjczedxwZLSoEOTGDjVNcjDu07x0Ksn2X+6l8wM46pVldy0YSE3rF+oaz6+k12D3PNMAz/ccZK+4TEuWFzKW8+v4fIVFayrKabQL6g4Oh7CYNZdbkoEklQ6+0f4wbbj3P/LI7T2DnPB4lI+c+Matq6p0k1i89T+0z089OopfranmaMdA5jBxcsWcNOGhVy3tppV1UVp97fvHhjl608f5lvPN4GDt19Yy+1X1rNxaVlc9qdEIIFzzvFCYwffe/k4j+05zch4iKtXV/Kxa1dy5cqKtDsJpCvnHAdaenlsTwuPv36avad6AKgpyePq1ZVcs6aKq1ZVUl6YE3Ck8TM0Os4DLzTx9aca6Bka5T0XL+EzN6xhUVl+XPerRCCBOdLez6O7m3lw+wmOtPdTkpfFb1+8hNu2LGNtTXHQ4UnATnYN8tzBNp491MYvD7XTMzSGGVywuJTLV1RwaX05m+sWsGAeJIZQyPHQzpN86fGDnOwaZOvaKv7nW9dxXm1iCicqEUhCNbb18ejuZh7ZfZp9zd43vi315dx22VLedn4tedkaCiq/aTzk2HWii+cOtvPcoTZ2nehidNw7J62uLuLS5eVsqS9n07IylpUXpEwr0jnHY3tb+PITBznQ0ssFi0v547et48pVlQmNQ4kALxuHnMMBIefINNM45xgZGBnjxcYOnjnQxrOH2jnS3g/AxcvKuPmCWm6+oDbuzV6Zf4ZGx9l1vIttR8/w8pFOdhw9Q+/wGACl+dlcuKSUCxaXcuGSMi5cUkptaV5SJYfxkOOJ11v4+lOH2X2ymxVVhXz6LWu45YLaQObQUCIAPv/QHr7z4tGzz81gYXEei8ryWFZewIVLyti0rIz1i0p089I0eoZG2XH0DNuPnuGVpk52HO1iZDxEXnYGV6yo4No1Vdy4oUYnf4mp8ZBj/+kedh3vZvfJLnYd7+ZASy/jIe+8taAgmzULi1lXU8zamhLW1hSxZmExxQkehtzeN8yD20/wnReOcrJrkKXl+XzyzWt418ZFgX75VCIAnjrQyu4T3WQYmBnDo+Oc6h7iVNcgjW39nO4ZAiA3K4PL/JPZtWuqWFlVmFTfMhKtZ2iU/c29vH6qm9ebe3jthPfhc86b6vC82mL/5F/N5voF6vaRhBoaHef15h52n+hmX3MPB1p6OXi6l/6Iktm1pXnUVRSwvLKQuopC6isKqKsoZFl5wdmhmXMxHnIcbu3juUNtPL63hW1HOwk5uGJFBXdcWc9bzqtOit4HJYIonO4eYufxM7x0pJNnD7bR0OZ1byxZkM+1a6q4Zk0Vl6/Ku2MLAAAM1UlEQVSooDR/ft3k5Jyjd3iMlu4hTpwZ5Eh7P00d/Rxp935OnBk8u25FYQ7rF5Wwua6czfUL2Li0LCYfJJFYCoUcJ7sGOXC6lwMtvTS09XG0Y4Cm9n46+kd+bd3i3CyqS3JZWJLHwpI8qktyqSrKpSAni4KcTPJzMinM8f6Pj4yPMzLm6BoY4XTPEC09Qxxu7WPPyR4G/ak919UUc9OGGm65sJY1C5NrMIQSwSwc7xzgmYNtPHOwjecPt9M/Mk6GP5rhipWVXLmygkvqFiTNidA5x9BoiN7hUXqHxvyfUfr8xz1D3vK+4TE6+ob9/8jDtPQMMTBhwpHi3CzqKwupryxkXU0x6xeVsL62hOri3LRuHUnq6xka5VjHAE0d/RzvHKSlZ4jW3jc+C609w4yMR1deu7Io52y38oVLSrm0vpyl5clbdE+JYI5GxkLsOHaG5xs6eKGhnVePdTEWcpjBispCNiwqZW1NMcvKC1haXsCi0jxKC7KjutbgnGNkPMTQSIi+kTH/xD169uTdN/zGSb13yHu9J/L58BuPx0LT//0KczIpL8phYXEeC0vzqCnJY6H/jWhxWT71lYVUFObohC9pyTlHz9AYgyPjDIyMMTAyfvaLUk5WBtmZRml+NtXFeeRkBd/dMxNKBDHWPzzGK02d7Dzexd5TPew92c2p7qHfWK8gJ5Oi3CwyM4wMMzIyINOM0XHH0Og4g6PjDI2OE8X5m8wMozgvi6LcLIrzsinOy6JkwvOiPO9xSV6Wv663vDgvi+LcbIryvFhEJP1EmwgC6eMws7cCXwEygfucc18MIo6ZKMzNYuvaarZGVMbsHx7jxJlBjnUOcLp7kO7BUboGvG/04yFHyHnDVsdDjuzMDPJzMsjL8vog87K9n6LcTIrzsv2T+xsn9aK8LPKzM/UtXUTiLuGJwMwyga8DNwAngFfM7GHn3OuJjmWuCnOzWFtTrLtlRSSlBdHhtQU47JxrdM6NAP8GvDOAOEREhGASwWLgeMTzE/6yX2Nmd5rZNjPb1tbWlrDgRETSTRCJYLJO79+4dOqcu8c5t9k5t7mqanaTMoiIyPSCSAQngKURz5cApwKIQ0RECCYRvAKsNrPlZpYD3Ao8HEAcIiJCAKOGnHNjZvYJ4DG84aP3O+f2JjoOERHxBHIfgXPuUeDRIPYtIiK/LrXulxYRkZhLiRITZtYGHJ12xclVAu0xDCdWFNfMKK6ZUVwzk6xxwdxiq3POTTvsMiUSwVyY2bZoam0kmuKaGcU1M4prZpI1LkhMbOoaEhFJc0oEIiJpLh0SwT1BBzAFxTUzimtmFNfMJGtckIDY5v01AhERObd0aBGIiMg5pGwiMLNyM3vCzA75vxdMsd7PzazLzP5jwvLlZvaSv/33/XIXmFmu//yw/3p9nOK6w1/nkJnd4S8rNrOdET/tZvZ3/msfNLO2iNd+P1Fx+cufNrMDEfuv9pcHebwKzOwRM9tvZnvN7IsR68/qeJnZW/1/52Ezu2uS16f895rZH/vLD5jZTdG+ZzzjMrMbzGy7me32f18fsc2kf9MExVVvZoMR+/5mxDaX+PEeNrOvms18dqY5xPX+CZ/BkJlt9F9LxPG6xsx2mNmYmf3OhNem+mzO+XjhnEvJH+D/Anf5j+8C/maK9d4MvAP4jwnLfwDc6j/+JvDf/McfB77pP74V+H6s4wLKgUb/9wL/8YJJ1tsOXOM//iDwtXger3PFBTwNbJ5km8COF1AAXOevkwM8B7xttscLr+RJA7DCf79dwPpo/r3Aen/9XGC5/z6Z0bxnnOPaBCzyH58PnIzYZtK/aYLiqgf2TPG+LwNX4FUq/ln4b5qIuCascwHQmODjVQ9cCDwA/E6Un805HS/nXOq2CPAms/m2//jbwLsmW8k59yTQG7nMz5jXAw9Osn3k+z4IvHmGGTaauG4CnnDOdTrnzgBPAG+dEONqoBrv5BYLMYlrmvdN6PFyzg04554CcN4kRzvwqtnOVjSTJk31730n8G/OuWHn3BHgsP9+sZiIadZxOededc6Fq/vuBfLMLHeG+495XFO9oZnVAiXOuRecd5Z7gCk+2wmI6zbgezPc95zics41OedeA0ITtp30MxCj45XSiWChc64ZwP89k2ZaBdDlnBvzn0dOjnN24hz/9W5//VjGFc3kPLfhfUuJvJr/HjN7zcweNLOlzEws4vpnv0n8+YgPTVIcLzMrw2v5PRmxeKbHK5q/y1T/3qm2jWoipjjGFek9wKvOueGIZZP9TRMV13Ize9XMnjGzqyPWPzHNe8Y7rrD38puJIN7Ha6bbxuJ4BVN0Llpm9gugZpKX/mSubz3JMhfFa7GKK5rJeW4Ffi/i+U+B7znnhs3sY3jfZq6P3CDOcb3fOXfSzIqBH/qxPTDNNomICzPLwvvAftU51+gvnvZ4zXQ/06wz1fLJvmzNdKjeXOLyXjTbAPwNcGPE61P9TRMRVzOwzDnXYWaXAA/5MUY1cVUc4/JeNLsMGHDO7Yl4PRHHa6bbxuJ4JXcicM69ZarXzKzFzGqdc81+86h1Bm/dDpSZWZb/bSBycpzwxDkn/BNMKdAZ47hOAFsjni/B638Mv8dFQJZzbnvEPjsi1r8X70P9a+IZl3PupP+718y+i9fMfYAkOF5446wPOef+LmKf0x6vKfYz3aRJU/17z7XtXCdimktcmNkS4MfA7c65hvAG5/ibxj0uv6U77O9/u5k1AGv89SO79xJ+vHy3MqE1kKDjda5tt07Y9mlic7xSumvoYSB85fwO4CfRbuj/J3wKCF+Vj9w+8n1/B/jPCd0zsYjrMeBGM1tg3iiZG/1lYb/RN+mfJMN+C9g3g5jmFJeZZZlZpR9HNvB2IPxNKdDjZWZfwPsQfypyg1ker2gmTZrq3/swcKt5o1GWA6vxLuLFYiKmWcfld5k9Avyxc+5X4ZWn+ZsmIq4qM8v0978C73g1+t2DvWZ2ud/1cjsz+GzPNS4/ngzgd/H68PGXJep4TWXSz0CMjldKjxqqwOsPPuT/LveXbwbui1jvOaANGMTLnjf5y1fgfVAPA/8O5PrL8/znh/3XV8Qprg/7+zgMfGjCezQC6yYs+z94F/t24SWxdYmKCyjEG8H0mh/DV4DMoI8X3rcfh3eS3+n//P5cjhdwM3AQb3THn/jL/hL4ren+vXhdXQ3AASJGbkz2nrP4/z6ruIA/Bfojjs9OvOswU/5NExTXeyL+PjuAd0S852a8k2wD8DX8G18TEZf/2lbgxQnvl6jjdSneeaof6AD2TnfOiMXx0p3FIiJpLpW7hkREJAaUCERE0pwSgYhImlMiEBFJc0oEIiJpTolAUpqZjfu3/O81s11m9hl/HPi5tllkZg/6j7fahMq002wbWdV0v5l9OopttprZldHuQyTRkvrOYpEoDDrnwmWCq4Hv4t1g9mdTbeC8Imy/M9XrUfi+c+4TZlYBHDCzB51zx8+x/lagD3g+2h2YWaZzbnwOMYpETS0CmTecc63AncAnzFNvZs+ZV999R/hbub/81+4KNbMM8+q8V0U8Pxy+m3SK/XXg3dxT629TZWY/NLNX/J+rzKtz/zHg034r4moz+5ZF1Jo3sz7/91Yze8ovX7Dbj3Ofmd3rt3geN7N8f90/NLPXzSuq92+IzIESgcwrzis6l4F392wrcINz7mK8SpJfPcd2IeBfgPf7i94C7HLOtU+1jZktw7tD9TV/0VeALzvnLsW7c/Y+51wT3nwXX3bObXTOTVdWfAveHafr/eerga875zYAXf77gjd3wybn3IV4iUZk1tQ1JPNRuCJjNvA182aYGscranYu9+PVafk7vNv5/3mK9d5rZtcBa4GPOueG/OVvAdbbG9WJS8yrVDkTLztvPoOwI865nf7j7XgTl4CXfP7VzB4CHprhPkR+jVoEMq/4BczG8VoDnwZagIvw6rHknGtbv5+/xbzpHC/Dm+1pMt/3v6FfDXzJzMIltjOAK/xv/hudc4udc72TbD/mrxueJCkyrv4J60bOHTDOG1/ebgG+DlwCbPcraIrMihKBzBt+//438aaodHgXjZv9bp/fw5sqcDr34XUR/WC6i7XOuReA7wCf9Bc9DnwiIp6N/sNeILJl0IR3AgdvhqrsKOI6yx8VtdR5s7P9EVAGFM3kPUQiKRFIqssPDx8FfoF3Mv4L/7V/AO4wsxfxuoUmftuezMN4J9WpuoUm+hvgQ34X0B8Cm/0LuK/zRt/9T4F3hy8W482PcK2ZvYzX8ogmrkiZwL+Y2W7gVbzrD10zfA+Rs1R9VCSCmW3GO7FePe3KIvOE+hVFfGZ2F/DfeGPkkEhaUItARCTN6RqBiEiaUyIQEUlzSgQiImlOiUBEJM0pEYiIpDklAhGRNPf/AeK1ZWDbp3nHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72d21a2898>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def density_plot(data): \n",
    "    density = gaussian_kde(data)\n",
    "    xs = np.linspace(np.min(data),np.max(data),200)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    plt.plot(xs,density(xs))\n",
    "    plt.xlabel('Daily Returns')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "test = frames['20040108']\n",
    "test['DlyReturn'] = wins(test['DlyReturn'],-0.1,0.1)\n",
    "density_plot(test['DlyReturn'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Exposures and Factor Returns\n",
    "\n",
    "Recall that:\n",
    "\n",
    "$r_{i,t} = \\sum_{j=1}^{k} (\\beta_{i,j,t-2} \\times f_{j,t})$  \n",
    "where $i=1...N$ (N assets),   \n",
    "and $j=1...k$ (k factors).\n",
    "\n",
    "where $r_{i,t}$ is the return, $\\beta_{i,j,t-2}$ is the factor exposure, and $f_{j,t}$ is the factor return. Since we get the factor exposures from the Barra data, and we know the returns, it is possible to estimate the factor returns. In this notebook, we will use the Ordinary Least Squares (OLS) method to estimate the factor exposures, $f_{j,t}$, by using $\\beta_{i,j,t-2}$ as the independent variable, and $r_{i,t}$ as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formula(factors, Y):\n",
    "    L = [\"0\"]\n",
    "    L.extend(factors)\n",
    "    return Y + \" ~ \" + \" + \".join(L)\n",
    "\n",
    "def factors_from_names(n):\n",
    "    return list(filter(lambda x: \"USFASTD_\" in x, n))\n",
    "\n",
    "def estimate_factor_returns(df): \n",
    "    ## build universe based on filters \n",
    "    print(df.columns)\n",
    "    estu = df.loc[df.IssuerMarketCap > 1e9].copy(deep=True)\n",
    "  \n",
    "    ## winsorize returns for fitting \n",
    "    estu['DlyReturn'] = wins(estu['DlyReturn'], -0.25, 0.25)\n",
    "  \n",
    "    all_factors = factors_from_names(list(df))\n",
    "    form = get_formula(all_factors, \"DlyReturn\")\n",
    "    model = ols(form, data=estu)\n",
    "    results = model.fit()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Barrid', 'DlyReturn'], dtype='object')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'IssuerMarketCap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a403923b5f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfacret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_factor_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7f92331a6e70>\u001b[0m in \u001b[0;36mestimate_factor_returns\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m## build universe based on filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mestu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIssuerMarketCap\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m## winsorize returns for fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'IssuerMarketCap'"
     ]
    }
   ],
   "source": [
    "facret = {}\n",
    "\n",
    "for date in frames:\n",
    "    facret[date] = estimate_factor_returns(frames[date]).params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dates = sorted(list(map(lambda date: pd.to_datetime(date, format='%Y%m%d'), frames.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Alpha Factors\n",
    "\n",
    "We will now choose our alpha factors. Barra's factors include some alpha factors that we have seen before, such as:\n",
    "\n",
    "* **USFASTD_1DREVRSL** : Reversal\n",
    "\n",
    "* **USFASTD_EARNYILD** : Earnings Yield\n",
    "\n",
    "* **USFASTD_VALUE** : Value\n",
    "\n",
    "* **USFASTD_SENTMT** : Sentiment\n",
    "\n",
    "We will choose these alpha factors for now, but you are encouraged to come back to this later and try other factors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_factors = [\"USFASTD_1DREVRSL\", \"USFASTD_EARNYILD\", \"USFASTD_VALUE\", \"USFASTD_SENTMT\"]\n",
    "\n",
    "facret_df = pd.DataFrame(index = my_dates)\n",
    "\n",
    "for dt in my_dates: \n",
    "    for alp in alpha_factors: \n",
    "        facret_df.at[dt, alp] = facret[dt.strftime('%Y%m%d')][alp]\n",
    "\n",
    "for column in facret_df.columns:\n",
    "        plt.plot(facret_df[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Factor Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Previous Portfolio Holdings \n",
    "\n",
    "In order to optimize our portfolio we will use the previous day's holdings to estimate the trade size and transaction costs. In order to keep track of the holdings from the previous day we will include a column to hold the portfolio holdings of the previous day. These holdings of all our assets will be initialized to zero when the backtest first starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nas(df): \n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for numeric_column in numeric_columns: \n",
    "        df[numeric_column] = np.nan_to_num(df[numeric_column])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_holdings = pd.DataFrame(data = {\"Barrid\" : [\"USA02P1\"], \"h.opt.previous\" : np.array(0)})\n",
    "df = frames[my_dates[0].strftime('%Y%m%d')]\n",
    "\n",
    "df = df.merge(previous_holdings, how = 'left', on = 'Barrid')\n",
    "df = clean_nas(df)\n",
    "df.loc[df['SpecRisk'] == 0]['SpecRisk'] = median(df['SpecRisk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Universe Based on Filters (TODO)\n",
    "\n",
    "In the cell below, implement the function `get_universe` that creates a stock universe by selecting only those companies that have a market capitalization of at least 1 billion dollars **OR** that are in the previous day's holdings, even if on the current day, the company no longer meets the 1 billion dollar criteria.\n",
    "\n",
    "When creating the universe, make sure you use the `.copy()` attribute to create a copy of the data. Also, it is very important to make sure that we are not looking at returns when forming the portfolio! to make this impossible, make sure to drop the column containing the daily return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_universe(df):\n",
    "    \"\"\"\n",
    "    Create a stock universe based on filters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        All stocks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    universe : DataFrame\n",
    "        Selected stocks based on filters\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "universe = get_universe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(int(universe['DataDate'][1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors\n",
    "\n",
    "We will now extract both the risk factors and alpha factors. We begin by first getting all the factors using the `factors_from_names` function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = factors_from_names(list(universe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the function `setdiff` to just select the factors that we have not defined as alpha factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setdiff(temp1, temp2): \n",
    "    s = set(temp2)\n",
    "    temp3 = [x for x in temp1 if x not in s]\n",
    "    return temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors = setdiff(all_factors, alpha_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save the column that contains the previous holdings in a separate variable because we are going to use it later when we perform our portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = universe['h.opt.previous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix of Risk Factor Exposures\n",
    "\n",
    "Our dataframe contains several columns that we'll use as risk factors exposures.  Extract these and put them into a matrix.\n",
    "\n",
    "The data, such as industry category, are already one-hot encoded, but if this were not the case, then using `patsy.dmatrices` would help, as this function extracts categories and performs the one-hot encoding.  We'll practice using this package, as you may find it useful with future data sets.  You could also store the factors in a dataframe if you prefer.\n",
    "\n",
    "#### How to use patsy.dmatrices\n",
    "\n",
    "`patsy.dmatrices` takes in a formula and the dataframe.  The formula tells the function which columns to take.  The formula will look something like this:  \n",
    "`SpecRisk ~ 0 + USFASTD_AERODEF + USFASTD_AIRLINES + ...`  \n",
    "where the variable to the left of the ~ is the \"dependent variable\" and the others to the right are the independent variables (as if we were preparing data to be fit to a model).\n",
    "\n",
    "This just means that the `pasty.dmatrices` function will return two matrix variables, one that contains the single column for the dependent variable `outcome`, and the independent variable columns are stored in a matrix `predictors`.\n",
    "\n",
    "The `predictors` matrix will contain the matrix of risk factors, which is what we want.  We don't actually need the `outcome` matrix; it's just created because that's the way patsy.dmatrices works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = get_formula(risk_factors, \"SpecRisk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_matrix(formula, data): \n",
    "    outcome, predictors = patsy.dmatrices(formula, data)\n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = model_matrix(formula, universe)\n",
    "BT = B.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Specific Variance\n",
    "\n",
    "Notice that the specific risk data is in percent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['SpecRisk'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in order to get the specific variance for each stock in the universe we first need to multiply these values by `0.01`  and then square them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specVar = (0.01 * universe['SpecRisk']) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor covariance matrix (TODO)\n",
    "\n",
    "Note that we already have factor covariances from Barra data, which is stored in the variable `covariance`.  `covariance` is a dictionary, where the key is each day's date, and the value is a dataframe containing the factor covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance['20040102'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, implement the function `diagonal_factor_cov` to create the factor covariance matrix. Note that the covariances are given in percentage units squared.  Therefore you must re-scale them appropriately so that they're in decimals squared. Use the given `colnames` function to get the column names from `B`. \n",
    "\n",
    "When creating factor covariance matrix, you can store the factor variances and covariances, or just store the factor variances.  Try both, and see if you notice any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colnames(B):\n",
    "    if type(B) == patsy.design_info.DesignMatrix: \n",
    "        return B.design_info.column_names\n",
    "    if type(B) == pandas.core.frame.DataFrame: \n",
    "        return B.columns.tolist()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_factor_cov(date, B):\n",
    "    \"\"\"\n",
    "    Create the factor covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : string\n",
    "           date. For example 20040102\n",
    "        \n",
    "    B : patsy.design_info.DesignMatrix OR pandas.core.frame.DataFrame\n",
    "        Matrix of Risk Factors\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Fm : Numpy ndarray\n",
    "        factor covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "Fvar = diagonal_factor_cov(date, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Costs\n",
    "\n",
    "To get the transaction cost, or slippage, we have to multiply the price change due to market impact by the amount of dollars traded:\n",
    "\n",
    "$$\n",
    "\\mbox{tcost_{i,t}} = \\% \\Delta \\mbox{price}_{i,t} \\times \\mbox{trade}_{i,t}\n",
    "$$\n",
    "\n",
    "In summation notation it looks like this:  \n",
    "$$\n",
    "\\mbox{tcost}_{i,t} = \\sum_i^{N} \\lambda_{i,t} (h_{i,t} - h_{i,t-1})^2\n",
    "$$  \n",
    "where\n",
    "$$\n",
    "\\lambda_{i,t} = \\frac{1}{10\\times \\mbox{ADV}_{i,t}}\n",
    "$$\n",
    "\n",
    "Note that since we're dividing by ADV, we'll want to handle cases when ADV is missing or zero.  In those instances, we can set ADV to a small positive number, such as 10,000, which, in practice assumes that the stock is illiquid. In the code below if there is no volume information we assume the asset is illiquid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda(universe, composite_volume_column = 'ADTCA_30'):\n",
    "    universe.loc[np.isnan(universe[composite_volume_column]), composite_volume_column] = 1.0e4\n",
    "    universe.loc[universe[composite_volume_column] == 0, composite_volume_column] = 1.0e4 \n",
    "\n",
    "    adv = universe[composite_volume_column]\n",
    "    \n",
    "    return 0.1 / adv\n",
    "\n",
    "Lambda = get_lambda(universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Combination (TODO)\n",
    "\n",
    "In the code below create a matrix of alpha factors and return it from the function `get_B_alpha`. Create this matrix in the same way you created the matrix of risk factors, i.e. using the `get_formula` and `model_matrix` functions we have defined above. Feel free to go back and look at the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_B_alpha(alpha_factors, universe):\n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "B_alpha = get_B_alpha(alpha_factors, universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the matrix containing the alpha factors we will combine them by adding its rows. By doing this we will collapse the `B_alpha` matrix into a single alpha vector. We'll multiply by `1e-4` so that the expression of expected portfolio return, $\\alpha^T \\mathbf{h}$, is in dollar units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_vec(B_alpha):\n",
    "    \"\"\"\n",
    "    Create an alpha vecrtor\n",
    "\n",
    "    Parameters\n",
    "    ----------        \n",
    "    B_alpha : patsy.design_info.DesignMatrix \n",
    "        Matrix of Alpha Factors\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    alpha_vec : patsy.design_info.DesignMatrix \n",
    "        alpha vecrtor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "alpha_vec = get_alpha_vec(B_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Challenge\n",
    "\n",
    "You can also try to a more sophisticated method of alpha combination, by choosing the holding for each alpha based on the same metric of its performance, such as the factor returns, or sharpe ratio.  To make this more realistic, you can calculate a rolling average of the sharpe ratio, which is updated for each day.  Remember to only use data that occurs prior to the date of each optimization, and not data that occurs in the future.  Also, since factor returns and sharpe ratios may be negative, consider using a `max` function to give the holdings a lower bound of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function (TODO)\n",
    "\n",
    "The objective function is given by:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{h}) = \\frac{1}{2}\\kappa \\mathbf{h}_t^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{h}_t + \\frac{1}{2} \\kappa \\mathbf{h}_t^T \\mathbf{S} \\mathbf{h}_t - \\mathbf{\\alpha}^T \\mathbf{h}_t + (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})^T \\mathbf{\\Lambda} (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})\n",
    "$$\n",
    "\n",
    "Where the terms correspond to: factor risk + idiosyncratic risk - expected portfolio return + transaction costs, respectively. We should also note that $\\textbf{Q}^T\\textbf{Q}$ is defined to be the same as $\\textbf{BFB}^T$.  Review the lessons if you need a refresher of how we get $\\textbf{Q}$.\n",
    "\n",
    "Our objective is to minimize this objective function. To do this, we will use Scipy's optimization function:\n",
    "\n",
    "`scipy.optimize.fmin_l_bfgs_b(func, initial_guess, func_gradient)`\n",
    "\n",
    "where:\n",
    "\n",
    "* **func** : is the function we want to minimize\n",
    "\n",
    "* **initial_guess** : is out initial guess\n",
    "\n",
    "* **func_gradient** : is the gradient of the function we want to minimize\n",
    "\n",
    "So, in order to use the `scipy.optimize.fmin_l_bfgs_b` function we first need to define its parameters.\n",
    "\n",
    "In the code below implement the function `obj_func(h)` that corresponds to the objective function above that we want to minimize. We will set the risk aversion to be `1.0e-6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_aversion = 1.0e-6\n",
    "\n",
    "def get_obj_func(h0, risk_aversion, Q, specVar, alpha_vec, Lambda): \n",
    "    def obj_func(h):\n",
    "        # TODO: Implement\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    return obj_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient (TODO)\n",
    "\n",
    "Now that we can generate the objective function using `get_obj_func`, we can now create a similar function with its gradient. The reason we're interested in calculating the gradient is so that we can tell the optimizer in which direction, and how much, it should shift the portfolio holdings in order to improve the objective function (minimize variance, minimize transaction cost, and maximize expected portfolio return).\n",
    "\n",
    "Before we implement the function we first need to know what the gradient looks like. The gradient, or derivative of the objective function, with respect to the portfolio holdings h, is given by:  \n",
    "\n",
    "$$\n",
    "f'(\\mathbf{h}) = \\frac{1}{2}\\kappa (2\\mathbf{Q}^T\\mathbf{Qh}) + \\frac{1}{2}\\kappa (2\\mathbf{Sh}) - \\mathbf{\\alpha} + 2(\\mathbf{h}_{t} - \\mathbf{h}_{t-1}) \\mathbf{\\Lambda}\n",
    "$$\n",
    "\n",
    "In the code below, implement the function `grad(h)` that corresponds to the function of the gradient given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_func(h0, risk_aversion, Q, QT, specVar, alpha_vec, Lambda):\n",
    "    def grad_func(h):\n",
    "        # TODO: Implement\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    return grad_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize (TODO)\n",
    "\n",
    "Now that we can generate the objective function using `get_obj_func`, and its corresponding gradient using `get_grad_func` we are ready to minimize the objective function using Scipy's optimization function. For this, we will use out initial holdings as our `initial_guess` parameter.\n",
    "\n",
    "In the cell below, implement the function `get_h_star` that optimizes the objective function. Use the objective function (`obj_func`) and gradient function (`grad_func`) provided within `get_h_star` to optimize the objective function using the `scipy.optimize.fmin_l_bfgs_b` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_aversion = 1.0e-6\n",
    "\n",
    "Q = np.matmul(scipy.linalg.sqrtm(Fvar), BT)\n",
    "QT = Q.transpose()\n",
    "\n",
    "def get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda):\n",
    "    \"\"\"\n",
    "    Optimize the objective function\n",
    "\n",
    "    Parameters\n",
    "    ----------        \n",
    "    risk_aversion : int or float \n",
    "        Trader's risk aversion\n",
    "        \n",
    "    Q : patsy.design_info.DesignMatrix \n",
    "        Q Matrix\n",
    "        \n",
    "    QT : patsy.design_info.DesignMatrix \n",
    "        Transpose of the Q Matrix\n",
    "        \n",
    "    specVar: Pandas Series \n",
    "        Specific Variance\n",
    "        \n",
    "    alpha_vec: patsy.design_info.DesignMatrix \n",
    "        alpha vector\n",
    "        \n",
    "    h0 : Pandas Series  \n",
    "        initial holdings\n",
    "        \n",
    "    Lambda : Pandas Series  \n",
    "        Lambda\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    optimizer_result[0]: Numpy ndarray \n",
    "        optimized holdings\n",
    "    \"\"\"\n",
    "    obj_func = get_obj_func(h0, risk_aversion, Q, specVar, alpha_vec, Lambda)\n",
    "    grad_func = get_grad_func(h0, risk_aversion, Q, QT, specVar, alpha_vec, Lambda)\n",
    "    \n",
    "    # TODO: Implement \n",
    "    \n",
    "    return None\n",
    "\n",
    "h_star = get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have optimized our objective function we can now use, `h_star` to create our optimal portfolio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_portfolio = pd.DataFrame(data = {\"Barrid\" : universe['Barrid'], \"h.opt\" : h_star})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Exposures (TODO)\n",
    "\n",
    "We can also use `h_star` to calculate our portfolio's risk and alpha exposures.\n",
    "\n",
    "In the cells below implement the functions `get_risk_exposures` and `get_portfolio_alpha_exposure` that calculate the portfolio's risk and alpha exposures, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_exposures(B, BT, h_star):\n",
    "    \"\"\"\n",
    "    Calculate portfolio's Risk Exposure\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : patsy.design_info.DesignMatrix \n",
    "        Matrix of Risk Factors\n",
    "        \n",
    "    BT : patsy.design_info.DesignMatrix \n",
    "        Transpose of Matrix of Risk Factors\n",
    "        \n",
    "    h_star: Numpy ndarray \n",
    "        optimized holdings\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    risk_exposures : Pandas Series\n",
    "        Risk Exposures\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "risk_exposures = get_risk_exposures(B, BT, h_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_alpha_exposure(B_alpha, h_star):\n",
    "    \"\"\"\n",
    "    Calculate portfolio's Alpha Exposure\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    B_alpha : patsy.design_info.DesignMatrix \n",
    "        Matrix of Alpha Factors\n",
    "        \n",
    "    h_star: Numpy ndarray \n",
    "        optimized holdings\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    alpha_exposures : Pandas Series\n",
    "        Alpha Exposures\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return pd.Series(np.matmul(B_alpha.transpose(), h_star), index = colnames(B_alpha))\n",
    "\n",
    "portfolio_alpha_exposure = get_portfolio_alpha_exposure(B_alpha, h_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Costs (TODO)\n",
    "\n",
    "We can also use `h_star` to calculate our total transaction costs:\n",
    "$$\n",
    "\\mbox{tcost} = \\sum_i^{N} \\lambda_{i} (h_{i,t} - h_{i,t-1})^2\n",
    "$$\n",
    "\n",
    "In the cell below, implement the function `get_total_transaction_costs` that calculates the total transaction costs according to the equation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_transaction_costs(h0, h_star, Lambda):\n",
    "    \"\"\"\n",
    "    Calculate Total Transaction Costs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h0 : Pandas Series\n",
    "        initial holdings (before optimization)\n",
    "        \n",
    "    h_star: Numpy ndarray \n",
    "        optimized holdings\n",
    "        \n",
    "    Lambda : Pandas Series  \n",
    "        Lambda\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    total_transaction_costs : float\n",
    "        Total Transaction Costs\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "total_transaction_costs = get_total_transaction_costs(h0, h_star, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "We can now take all the above functions we created above and use them to create a single function, `form_optimal_portfolio` that returns the optimal portfolio, the risk and alpha exposures, and the total transactions costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_optimal_portfolio(df, previous, risk_aversion):\n",
    "    df = df.merge(previous, how = 'left', on = 'Barrid')\n",
    "    df = clean_nas(df)\n",
    "    df.loc[df['SpecRisk'] == 0]['SpecRisk'] = median(df['SpecRisk'])\n",
    "  \n",
    "    universe = get_universe(df)\n",
    "    date = str(int(universe['DataDate'][1]))\n",
    "  \n",
    "    all_factors = factors_from_names(list(universe))\n",
    "    risk_factors = setdiff(all_factors, alpha_factors)\n",
    "  \n",
    "    h0 = universe['h.opt.previous']\n",
    "  \n",
    "    B = model_matrix(get_formula(risk_factors, \"SpecRisk\"), universe)\n",
    "    BT = B.transpose()\n",
    "  \n",
    "    specVar = (0.01 * universe['SpecRisk']) ** 2\n",
    "    Fvar = diagonal_factor_cov(date, B)\n",
    "    \n",
    "    Lambda = get_lambda(universe)\n",
    "    B_alpha = get_B_alpha(alpha_factors, universe)\n",
    "    alpha_vec = get_alpha_vec(B_alpha)\n",
    "  \n",
    "    Q = np.matmul(scipy.linalg.sqrtm(Fvar), BT)\n",
    "    QT = Q.transpose()\n",
    "    \n",
    "    h_star = get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda)\n",
    "    opt_portfolio = pd.DataFrame(data = {\"Barrid\" : universe['Barrid'], \"h.opt\" : h_star})\n",
    "    \n",
    "    risk_exposures = get_risk_exposures(B, BT, h_star)\n",
    "    portfolio_alpha_exposure = get_portfolio_alpha_exposure(B_alpha, h_star)\n",
    "    total_transaction_costs = get_total_transaction_costs(h0, h_star, Lambda)\n",
    "  \n",
    "    return {\n",
    "        \"opt.portfolio\" : opt_portfolio, \n",
    "        \"risk.exposures\" : risk_exposures, \n",
    "        \"alpha.exposures\" : portfolio_alpha_exposure,\n",
    "        \"total.cost\" : total_transaction_costs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tradelist\n",
    "\n",
    "The trade list is the most recent optimal asset holdings minus the previous day's optimal holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tradelist(prev_holdings, opt_result):\n",
    "    tmp = prev_holdings.merge(opt_result['opt.portfolio'], how='outer', on = 'Barrid')\n",
    "    tmp['h.opt.previous'] = np.nan_to_num(tmp['h.opt.previous'])\n",
    "    tmp['h.opt'] = np.nan_to_num(tmp['h.opt'])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save optimal holdings as previous optimal holdings.\n",
    "\n",
    "As we walk through each day, we'll re-use the column for previous holdings by storing the \"current\" optimal holdings as the \"previous\" optimal holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_previous(result): \n",
    "    prev = result['opt.portfolio']\n",
    "    prev = prev.rename(index=str, columns={\"h.opt\": \"h.opt.previous\"}, copy=True, inplace=False)\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the backtest\n",
    "\n",
    "Walk through each day, calculating the optimal portfolio holdings and trade list.  This may take some time, but should finish sooner if you've chosen all the optimizations you learned in the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = {}\n",
    "port = {}\n",
    "\n",
    "for dt in tqdm(my_dates, desc='Optimizing Portfolio', unit='day'):\n",
    "    date = dt.strftime('%Y%m%d')\n",
    "\n",
    "    result = form_optimal_portfolio(frames[date], previous_holdings, risk_aversion)\n",
    "    trades[date] = build_tradelist(previous_holdings, result)\n",
    "    port[date] = result\n",
    "    previous_holdings = convert_to_previous(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profit-and-Loss (PnL) attribution (TODO)\n",
    "\n",
    "Profit and Loss is the aggregate realized daily returns of the assets, weighted by the optimal portfolio holdings chosen, and summed up to get the portfolio's profit and loss.\n",
    "\n",
    "The PnL attributed to the alpha factors equals the factor returns times factor exposures for the alpha factors.  \n",
    "\n",
    "$$\n",
    "\\mbox{PnL}_{alpha}= f \\times b_{alpha}\n",
    "$$\n",
    "\n",
    "Similarly, the PnL attributed to the risk factors equals the factor returns times factor exposures of the risk factors.\n",
    "\n",
    "$$\n",
    "\\mbox{PnL}_{risk} = f \\times b_{risk}\n",
    "$$\n",
    "\n",
    "In the code below, in the function `build_pnl_attribution` calculate the PnL attributed to the alpha factors, the PnL attributed to the risk factors, and attribution to cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assumes v, w are pandas Series \n",
    "def partial_dot_product(v, w):\n",
    "    common = v.index.intersection(w.index)\n",
    "    return np.sum(v[common] * w[common])\n",
    "\n",
    "def build_pnl_attribution(): \n",
    "\n",
    "    df = pd.DataFrame(index = my_dates)\n",
    "    \n",
    "    for dt in my_dates:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "\n",
    "        p = port[date]\n",
    "        fr = facret[date]\n",
    "\n",
    "        mf = p['opt.portfolio'].merge(frames[date], how = 'left', on = \"Barrid\")\n",
    "        \n",
    "        mf['DlyReturn'] = wins(mf['DlyReturn'], -0.5, 0.5)\n",
    "        df.at[dt,\"daily.pnl\"] = np.sum(mf['h.opt'] * mf['DlyReturn'])\n",
    "        \n",
    "        # TODO: Implement\n",
    "    \n",
    "        df.at[dt,\"attribution.alpha.pnl\"] = \n",
    "        df.at[dt,\"attribution.risk.pnl\"] = \n",
    "        df.at[dt,\"attribution.cost\"] = \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = build_pnl_attribution()\n",
    "\n",
    "for column in attr.columns:\n",
    "        plt.plot(attr[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PnL Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build portfolio characteristics (TODO)\n",
    "Calculate the sum of long positions, short positions, net positions, gross market value, and amount of dollars traded.\n",
    "\n",
    "In the code below, in the function `build_portfolio_characteristics` calculate the sum of long positions, short positions, net positions, gross market value, and amount of dollars traded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_portfolio_characteristics(): \n",
    "    df = pd.DataFrame(index = my_dates)\n",
    "    \n",
    "    for dt in my_dates:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "  \n",
    "        p = port[date]\n",
    "        tradelist = trades[date]\n",
    "        h = p['opt.portfolio']['h.opt']\n",
    "        \n",
    "        # TODO: Implement\n",
    "        \n",
    "        df.at[dt,\"long\"] = \n",
    "        df.at[dt,\"short\"] = \n",
    "        df.at[dt,\"net\"] = \n",
    "        df.at[dt,\"gmv\"] = \n",
    "        df.at[dt,\"traded\"] = \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pchar = build_portfolio_characteristics()\n",
    "\n",
    "for column in pchar.columns:\n",
    "        plt.plot(pchar[column], label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional\n",
    "Choose additional metrics to evaluate your portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
